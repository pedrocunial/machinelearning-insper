{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(list(iris.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade (para fazer agora):** Construa um classificador por regressão logística para separar as flores do tipo 'Iris Virginica' das demais usando as características 'petal length (cm)' e 'petal width (cm)'. Como resultado final, apresente:\n",
    "\n",
    "- Acurácia do classificador no conjunto de testes.\n",
    "- Curva ROC e respectiva área.\n",
    "- Um diagrama ilustrando a probabilidade da classe positiva. \n",
    "    - Dica: veja https://matplotlib.org/gallery/images_contours_and_fields/contour_demo.html\n",
    "\n",
    "Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "X = X[['petal length (cm)', 'petal width (cm)']]\n",
    "y = (y == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pos = X_train[y_train]\n",
    "X_neg = X_train[~y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f1 = 'petal length (cm)'\n",
    "f2 = 'petal width (cm)'\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(X_pos[f1], X_pos[f2], 'ro')\n",
    "plt.plot(X_neg[f1], X_neg[f2], 'bo')\n",
    "plt.xlabel(f1)\n",
    "plt.ylabel(f2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=1)),\n",
    "    ('clf', LogisticRegression(penalty='l1'))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'clf__C': [2**k for k in range(-10, 11)],\n",
    "    'clf__penalty': ['l1', 'l2']\n",
    "}\n",
    "clf = GridSearchCV(pipe, params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scores = clf.predict_proba(X_test)\n",
    "y_scores = y_scores[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.arange(1, 7, 0.01)\n",
    "v = np.arange(0, 3.0, 0.01)\n",
    "U, V = np.meshgrid(u, v)\n",
    "\n",
    "X_test_plot = np.c_[U.reshape(U.size, 1), V.reshape(V.size, 1)]\n",
    "y_test_plot = clf.predict_proba(X_test_plot)\n",
    "y_test_plot = y_test_plot[:,1].reshape(U.shape)\n",
    "\n",
    "X_test_pos = X_test[y_test]\n",
    "X_test_neg = X_test[~y_test]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "CS = plt.contour(U, V, y_test_plot)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "\n",
    "plt.plot(X_test_pos[f1], X_test_pos[f2], 'ro')\n",
    "plt.plot(X_test_neg[f1], X_test_neg[f2], 'bo')\n",
    "plt.xlabel(f1)\n",
    "plt.ylabel(f2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade (para casa):** Repita a atividade anterior usando todas as quatro características originais. Qual o aumento de desempenho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade (para fazer agora):** Repita a atividade de classificação do dataset 'Iris' usando apenas as características 'petal length (cm)' e 'petal width (cm)'. Como resultado final, apresente:\n",
    "\n",
    "- Acurácia do classificador no conjunto de testes.\n",
    "- Diagramas ilustrando a probabilidade para cada classe\n",
    "    - Dica: veja https://matplotlib.org/gallery/images_contours_and_fields/contour_demo.html\n",
    "\n",
    "Use seu arsenal de ferramentas de validação para encontrar o melhor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade (para casa):** Repita a atividade anterior usando todas as quatro características originais. Qual o aumento de desempenho?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "(com material adaptado do livro texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(clf, axes):\n",
    "    # Constroi uma lista de valores das variáveis independentes\n",
    "    # que cubra o espaço amostral.\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    \n",
    "    # Constroi as predições (binárias) e a função de decisão (contínua).\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "\n",
    "    # Desenha a curva de decisão e as curvas de nível da função de decisão.\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "    \n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\")\n",
    "    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=0.001, loss=\"hinge\", random_state=42))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from pprint import pprint\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    polynomial_svm_clf, \n",
    "    {\n",
    "        'svm_clf__C' : [2**k for k in range(-20, 21, 2)]\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=ShuffleSplit(n_splits=50, test_size=0.25, random_state=RANDOM_SEED)\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plot_predictions(grid, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** \n",
    "- Como funciona e para que serve o ShuffleSplit que eu usei no GridSearchCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analisar a acurácia real do classificador usando validação cruzada, e alterando o parâmetro de regularização $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P = np.array([p['svm_clf__C'] for p in grid.cv_results_['params']])\n",
    "M = grid.cv_results_['mean_test_score']\n",
    "S = grid.cv_results_['std_test_score']\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print('{}: mean_accuracy = {}, stddev_accuracy = {}'.format(p, m, s))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('Accuracy from CV', fontsize=20)\n",
    "plt.xlabel(r\"$C$\", fontsize=20)\n",
    "plt.ylabel(r\"$y$\", fontsize=20, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** \n",
    "- Explique o gráfico acima em termos do tradeoff bias/variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Teste corretamente o desempenho dos classificadores abaixo no dataset anterior (moons)\n",
    "\n",
    "- LinearSVC\n",
    "- SVC, com kernel:\n",
    "    - polinomial\n",
    "    - RBF\n",
    "   \n",
    "Apresente os seguintes resultados:\n",
    "\n",
    "- parâmetros ótimos\n",
    "- Acurácia\n",
    "- Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar support vector machines para regressão também.\n",
    "\n",
    "- Em problemas de classificação com SVM queremos construir uma fronteira de decisão tal que a \"avenida de separação\" entre classes é a maior possível. Dentro desta \"avenida\" queremos o menor número de pontos possível.\n",
    "\n",
    "- A idéia de usar SVM para regressão é o contrário: queremos construir um ajuste de função tal que a \"avenida\" contenha o **maior** número de pontos possível, para uma dada largura!\n",
    "\n",
    "Vamos ilustrar estes pontos com um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1)\n",
    "svm_poly_reg1.fit(X, y)\n",
    "svm_poly_reg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
    "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
    "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.legend(loc=\"upper left\", fontsize=18)\n",
    "    plt.axis(axes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n",
    "plt.title(r\"degree={}, C={}, $\\epsilon$ = {}\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon), fontsize=18)\n",
    "plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
    "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Explique o efeito do parâmetro de regularização C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos praticar usando o dataset \"California Housing\" do scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(housing['data'], columns=housing['feature_names'])\n",
    "y = pd.Series(housing['target'], name='MedHouseValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y >= 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(figsize=(15,15), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tem uns outliers malucos aparentemente! Onde já se viu um distrito onde a ocupação média dos imóveis é mais de 1000 pessoas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.AveOccup > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Descubra o que aconteceu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Parece que essas \"casas\" tem ocupação alta mesmo, ainda mais nos Estados Unidos (e no Brasil). \n",
    " \n",
    "Temos um problema também em relação ao número de cômodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.AveRooms > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Explique esse fenômeno também"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para não misturar tipos de \"residências\", vamos filtrar o dataset e eliminar alguns outliers. Vamos nos restringir a um número de cômodos menor que 15, e uma ocupação média menor que 10. \n",
    "\n",
    "Vamos também eliminar os distritos onde o valor mediano dos imóveis excede $5.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid = (X.AveRooms[:] < 15) & (X.AveOccup < 10) & (y < 5.0)\n",
    "X_filt = X[valid]\n",
    "y_filt = y[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filt.hist(figsize=(12, 12), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_filt.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filt, y_filt, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines são bastante eficientes para conjuntos de dados pequenos, mas seu processo de treinamento é extremamente lento. Para escolher o valor ótimo do parâmetro de regularização $C$ vamos reamostrar os dados e fazer a busca por validação cruzada em um dataset pequeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X_train_small, y_train_small = resample(X_train, y_train, replace=False, n_samples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', SVR(C=1, epsilon=0.1))\n",
    "    ]),\n",
    "    {\n",
    "        'reg__C': [10**k for k in range(-4, 7)]\n",
    "    },\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "grid.fit(X_train_small, y_train_small)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [p['reg__C'] for p in grid.cv_results_['params']]\n",
    "M = -grid.cv_results_['mean_test_score']\n",
    "S = np.log(grid.cv_results_['std_test_score'] + 1)\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(p, m, s)\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('MSE from CV', fontsize=20)\n",
    "plt.xlabel(r\"$C$\", fontsize=20)\n",
    "plt.ylabel(r\"$y$\", fontsize=20, rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Mais uma vez, explique esse gráfico em termos do tradeoff bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar também ajustar o parâmetro $\\gamma$ do modelo (ver 'Gaussian RBF Kernel' no livro texto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', SVR(C=1, epsilon=0.1, gamma=0.1))\n",
    "    ]),\n",
    "    {\n",
    "        'reg__gamma': [10**k for k in range(-7, 5)]\n",
    "    },\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "grid.fit(X_train_small, y_train_small)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "P = [p['reg__gamma'] for p in grid.cv_results_['params']]\n",
    "M = -grid.cv_results_['mean_test_score']\n",
    "S = np.log(grid.cv_results_['std_test_score'] + 1)\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(p, m, s)\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('MSE from CV', fontsize=20)\n",
    "plt.xlabel(r\"$\\gamma$\", fontsize=20)\n",
    "plt.ylabel(r\"$y$\", fontsize=20, rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** yadda yadda yadda tradeoff bias/variance you know what to do :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos testar o desempenho final do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = grid.best_estimator_\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar com um regressor linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred_lin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que o regressor SVM é mais preciso, mas requer um tempo de treinamento várias ordens de magnitude maior que o regressor linear. \n",
    "\n",
    "**Atividade:** A unidade de medida da variável dependente é \"dezenas de milhares de dólares\". Explique para seu chefe porque você merece uma promoção pelo seu trabalho com um regressor SVM enquanto seu colega (que não fez a disciplina de Machine Learning) usou uma regressão linear simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividades:**\n",
    "\n",
    "- Qual a idéia fundamental das Support Vector Machines? O que são vetores de suporte?\n",
    "\n",
    "- (Desafio) A segunda idéia mais importante das SVMs é o uso de kernels. Os kernels permitiram a expansão das SVMs para além dos modelos lineares. Em particular, o kernel RBF (radial-basis function) é bastante popular entre os usuários de SVMs, e apresenta desempenho bem elevado em geral. O que são kernels? Qual a sua relação com o problema de otimização dual das SVMs?\n",
    "\n",
    "- Se dobrarmos o número de features em uma modelagem SVM, quanto sobe o tempo de treinamento de um classificador SVM linear? E de um SVM RBF?\n",
    "\n",
    "- Se dobrarmos o número de amostras de treinamento de um classificador SVM linear, quanto sobe o tempo de treinamento? E se for um classificador SVM RBF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
